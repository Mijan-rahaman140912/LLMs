{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Implementing a GPT model from scratch to generate text"
      ],
      "metadata": {
        "id": "fwfSXUMPnhrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Coding an LLM architecture"
      ],
      "metadata": {
        "id": "4DVphtUnFSWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_128M = {\n",
        "    \"vocab_size\": 500257,      # vocabulary size\n",
        "    \"context_length\": 1024,    #context length\n",
        "    \"emb_dim\": 768,            # Embedding dimension\n",
        "    \"n_heads\": 12,             # number of attention head\n",
        "    \"n_layers\": 12,            # number of layers\n",
        "    \"drop_rate\": 0.1,          # dropout rate\n",
        "    \"qkv_bias\":False           # Query-key-value bias\n",
        "}\n",
        "cfg = GPT_CONFIG_128M"
      ],
      "metadata": {
        "id": "uBJjCHh_F9xp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "MWHxD-2WWAFS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uI3Q97_nFIje"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb =nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb =nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    # Use a placeholder for TransformerBlock\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[DummyTransformerBlock(cfg) for _ in range (cfg[\"n_layers\"])])\n",
        "\n",
        "    # Use a place holder for LayerNorm\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len, = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  class DummyTransformerBlock(nn.Module):\n",
        "     def __init__(self, cfg):\n",
        "         super().__init__()\n",
        "        # A simple placeholder\n",
        "\n",
        "\n",
        "     def forward(self,x):\n",
        "      # This block does nothing and returns its input\n",
        "        return x\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-KyElMD1RJOq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  class DummyLayerNorm(nn.Module):\n",
        "      def __init__(self, normalized_shape, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # Parameters here are just mimic the LayerNorm interface\n",
        "\n",
        "      def forward(self, x):\n",
        "        # This block does nothing and returns its input\n",
        "        return x"
      ],
      "metadata": {
        "id": "QjIninjhRMCG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import tiktoken\n",
        "\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  batch = []\n",
        "  txt1 = \"Every effort moves you\"\n",
        "  txt2 = \"Every day holds a\"\n",
        "\n",
        "  batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "  batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "  batch = torch.stack(batch,dim=0)\n",
        "  print(batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfevF0EHVI4s",
        "outputId": "cda955da-7a4b-4023-b07c-467b019c1ee8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(cfg)\n",
        "logits = model(batch)\n",
        "print(\"output shape:\",logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY2g-6xzWO4E",
        "outputId": "01124c5d-81d5-426c-9771-3c3dd9398739"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output shape: torch.Size([2, 4, 500257])\n",
            "tensor([[[ 0.2593, -0.9350, -0.6097,  ...,  1.2670, -0.8415,  0.4850],\n",
            "         [-0.1641, -0.5332,  1.0188,  ...,  0.3370,  0.3371,  0.8372],\n",
            "         [-0.4035,  1.2549,  1.3933,  ...,  0.3039, -1.2841,  0.3997],\n",
            "         [ 0.3540,  1.3452, -0.3613,  ...,  0.1178, -1.0502, -0.4579]],\n",
            "\n",
            "        [[ 0.6703, -0.7034, -0.2323,  ...,  1.1014, -1.0304,  0.7107],\n",
            "         [-1.6335, -0.7888,  0.0065,  ..., -0.1306, -0.0326,  0.1084],\n",
            "         [-0.3380,  1.7104,  0.5594,  ...,  0.2230, -0.7844, -0.0983],\n",
            "         [-0.3292,  0.3263,  1.0420,  ...,  0.2344,  0.7905,  0.5320]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 Normalizing activations with layer normalization"
      ],
      "metadata": {
        "id": "G3rE3EnZmmRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "batch_example = torch.randn(2,5)\n",
        "batch_example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iv-V4-AbD-3",
        "outputId": "acdbbce1-a374-4d43-d9f4-2195028b7a38"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
              "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer = nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
        "out = layer(batch_example)\n"
      ],
      "metadata": {
        "id": "JPhLFyO6ouft"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DImVrW_pD3J",
        "outputId": "50ddf6f0-8b5e-4ae7-9cb5-481be3fdaa6b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
              "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
              "       grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean = out.mean(dim=-1, keepdim=True)\n",
        "mean"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_INZpZupV3k",
        "outputId": "4a0f0914-1979-4a03-8871-3fc5e493eb45"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1324],\n",
              "        [0.2170]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var = out.var(dim=-1, keepdim=True)\n",
        "var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7NFc1sTp_8u",
        "outputId": "7f2e7a66-75b9-43a3-ad26-d0ca6f68bea9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0231],\n",
              "        [0.0398]], grad_fn=<VarBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(out - mean).mean(dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8XoC0Yhq5Fc",
        "outputId": "793c6fed-fa44-44eb-b580-063dee0ecf67"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    -0.0000,      0.0000], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_printoptions(sci_mode=False)"
      ],
      "metadata": {
        "id": "aPiR4Sb7rP5l"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normed = ((out - mean) /torch.sqrt(var))\n",
        "normed.var(dim=1, keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7idHbsm1reHE",
        "outputId": "e2ef3433-1685-492e-bd6f-212cebeaee5e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000],\n",
              "        [1.0000]], grad_fn=<VarBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormal(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.esp = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=True)\n",
        "    norm_x = ((x - mean) / torch.sqrt(var))\n",
        "    return self.scale * ((x - mean) / torch.sqrt(var +self.esp)) + self.shift"
      ],
      "metadata": {
        "id": "keW1qZWAwFOy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNormal(6)\n",
        "outputs_normed = ln(out)\n",
        "outputs_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkOOL4OpzABa",
        "outputId": "29f8ea5b-9e0e-4c0e-be56-9bfb137dc454"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6157,  1.4123, -0.8717,  0.5871, -0.8717, -0.8717],\n",
              "        [-0.0189,  0.1121, -1.0875,  1.5171,  0.5647, -1.0875]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 Implementing a feed forward network with GELU activations"
      ],
      "metadata": {
        "id": "wFV93s7n08_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU (nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "      return 0.5 * x * (1 + torch.tanh(torch.sqrt(2 / torch.pi) *\n",
        "       (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "AN8H3n7b-QUL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
        "    nn.GELU(),\n",
        "    nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    return self.layer(x)\n",
        ""
      ],
      "metadata": {
        "id": "wK5Pb4O3zX6V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn = FeedForward(cfg)\n"
      ],
      "metadata": {
        "id": "YDZLGA3v8Cnb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.randn(2,3,768)\n",
        "ffn(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhQywPTR8a2A",
        "outputId": "5ee2aab4-2511-4f21-8f0c-64fd43f26a31"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.4 Adding shortcut connection"
      ],
      "metadata": {
        "id": "f07wnXSg-gn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_size, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([\n",
        "         nn.Sequential(nn.Linear(layer_size[i], layer_size[i+1]), nn.GELU())\n",
        "         for i in range(len(layer_size) - 1)\n",
        "          ])\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x)\n",
        "      #  Check if shortcut can be applied\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x\n",
        "\n",
        "\n",
        "def print_gradients(model,x):\n",
        "    # forward pass\n",
        "    output = model(x)\n",
        "    target = torch.tensor(0.)\n",
        "\n",
        "    # calculate loss based on how close the target and output are\n",
        "    loss = nn.MSELoss()\n",
        "    loss = loss(output, target)\n",
        "\n",
        "    # Backward pass to calculate the gradients\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "      if 'weight' in name:\n",
        "        # print mean absolute gradient of the weights\n",
        "        print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "h2Zq5iNc8gc6"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_size = [3,3,3,3,3,1]\n",
        "x= torch.tensor([1., 0.,-1])"
      ],
      "metadata": {
        "id": "qi0VpT5WGc_7"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(layer_size, use_shortcut=False)\n",
        "\n",
        "print_gradients(model_without_shortcut, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q9ntlJWHJeQ",
        "outputId": "a415e36a-e036-475c-a0ef-2d6a236f8086"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.00020174118981231004\n",
            "layers.1.0.weight has gradient mean of 0.00012011769285891205\n",
            "layers.2.0.weight has gradient mean of 0.0007152436301112175\n",
            "layers.3.0.weight has gradient mean of 0.00139885104727\n",
            "layers.4.0.weight has gradient mean of 0.005049602594226599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(layer_size, use_shortcut=True)\n",
        "\n",
        "print_gradients(model_without_shortcut, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQnYHtEbKGeZ",
        "outputId": "b9cfc311-6aaa-40e0-f38c-9a35a893b546"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.22186800837516785\n",
            "layers.1.0.weight has gradient mean of 0.20709273219108582\n",
            "layers.2.0.weight has gradient mean of 0.3292388319969177\n",
            "layers.3.0.weight has gradient mean of 0.2667772173881531\n",
            "layers.4.0.weight has gradient mean of 1.3268063068389893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.5 Connecting attention and layers in a transformer block"
      ],
      "metadata": {
        "id": "9DGAxYEIKTXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in = cfg[\"emb_dim\"],\n",
        "        d_out = cfg[\"emb_dim\"],\n",
        "        context_length =cfg[\"context_length\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        dropout = cfg[\"drop_rate\"],\n",
        "        qkv_bias = cfg[\"qkv_bias\"]\n",
        "        )\n",
        "    self.ffn = FeedForward(cfg)\n",
        "    self.norm1 = LayerNormal(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNormal(cfg[\"emb_dim\"])\n",
        "    self.drop_out = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x =self.att(x)\n",
        "    x = self.drop_out(x)\n",
        "    x = x + shortcut\n",
        "    x = self.norm2(x)\n",
        "    x = self.ffn(x)\n",
        "    x = self.drop_out(x)\n",
        "    x = x + shortcut\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "-Qb-9KuGHmmt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.randn(2,4, 768)\n",
        "block = TransformerBlock(cfg)\n",
        "block(x).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GhRM_YUQwab",
        "outputId": "84bb78d4-6769-49d4-c24a-41c690834b13"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from chapter 3"
      ],
      "metadata": {
        "id": "4hwML5usOqDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in, d_out,context_length, dropout, num_heads=2, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert(d_out % num_heads == 0), \\\n",
        "        \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads # reduce the projection dim to match desired output dim\n",
        "\n",
        "    self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = torch.nn.Linear(d_out, d_out)\n",
        "    self.dropout = torch.nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape # Shape : (b, num_tokens, d_out)\n",
        "    queries = self.W_query(x)\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # we implicitly split the matrix by adding a ` num_heads` dimention\n",
        "    # unroll llast dim : (b, num_tokens, d_out) -->(b, num_tokens,  head_dim)\n",
        "\n",
        "    keys = keys.reshape(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.reshape(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.reshape(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    #Transpose: ((b, num_tokens, num_heads, head_dim) -->(b, num_heads, num_tokens,  head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    # Compute scaled dot-product attention (selfattention) with a causal mask\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3) # dot product for each head\n",
        "\n",
        "    # Original mask truncated to the number of tokens and converted to boolean\n",
        "\n",
        "    mask_bool = self.mask[:num_tokens, :num_tokens].bool()\n",
        "\n",
        "    # Use the mask to fill attention scores\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "\n",
        "    # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "\n",
        "    contex_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "IkYV728LOlfx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.6 Coding the GPT model"
      ],
      "metadata": {
        "id": "i2szkSPpSIuj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYmbsl8cSOMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  import tiktoken\n",
        "\n",
        "\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  batch = []\n",
        "  txt1 = \"Every effort moves you\"\n",
        "  txt2 = \"Every day holds a\"\n",
        "\n",
        "  batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "  batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "  batch = torch.stack(batch,dim=0)\n",
        "  print(batch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "934c0c02-2577-4742-dc0f-e002c10e72bf",
        "id": "qG-wNSlPUp5T"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RNX4jKLiVNoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_128M = {\n",
        "    \"vocab_size\": 500257,      # vocabulary size\n",
        "    \"context_length\": 1024,    #context length\n",
        "    \"emb_dim\": 768,            # Embedding dimension\n",
        "    \"n_heads\": 12,             # number of attention head\n",
        "    \"n_layers\": 12,            # number of layers\n",
        "    \"drop_rate\": 0.1,          # dropout rate\n",
        "    \"qkv_bias\":False           # Query-key-value bias\n",
        "}\n",
        "cfg = GPT_CONFIG_128M"
      ],
      "metadata": {
        "id": "mWV3X8ZCVRMX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zos4Ni15SlaA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb =nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb =nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range (cfg[\"n_layers\"])])\n",
        "\n",
        "    self.final_norm = LayerNormal(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len, = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "model = GPTModel(cfg)\n",
        "out = model(batch)"
      ],
      "metadata": {
        "id": "MbLJ5srUTs4M"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-nDTVzUT6iz",
        "outputId": "ec11b3af-9213-40b7-d769-8b43b163b154"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 500257])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGTDvXijVnF_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}